import os
import numpy as np
import json
from transformers import AutoTokenizer, AutoModel
import torch
import faiss
from tqdm import tqdm               

# ğŸ”¹ 1. é¸æ“‡åµŒå…¥æ¨¡å‹ï¼ˆå»ºè­°ç”¨ BGE-M3 æˆ– Sentence-BERTï¼‰
# model_name = "BAAI/bge-small-en"  
# model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_name = 'microsoft/codebert-base'
# model_name = "jackaduma/SecBERT"
# model_name = "cssupport/mobilebert-sql-injection-detect"
# model_name = "roberta-base-openai-detector"


training = "xss_dataset.json"


print(f"ğŸ” ä½¿ç”¨æ¨¡å‹: {model_name}")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
model_filename = model_name.replace('-', '_').replace('/', '_')

# ğŸ”¹ 2. è¨­å®šè³‡æ–™èˆ‡è¼¸å‡ºç›®éŒ„
base_output_dir = "../../dataset/vector"
model_output_dir = os.path.join(base_output_dir, model_name.replace('-', '_').replace('/', '_'))
os.makedirs(model_output_dir, exist_ok=True)

# ğŸ”¹ 3. è®€å– XSS è³‡æ–™é›†
with open(f"../../dataset/json/{training}", "r", encoding="utf-8") as f:
    dataset = json.load(f)

# å–å¾— Payloads èˆ‡ Labels
payloads = [item["Payload"] for item in dataset]  # XSS å…§å®¹
labels = [item["Label"] for item in dataset]  # 0=åˆæ³•, 1=éæ³•

# ç¢ºä¿æ•¸æ“šä¸€è‡´
assert len(payloads) == len(labels), "âŒ æ•¸æ“šæ•¸é‡ä¸ä¸€è‡´ï¼Œè«‹æª¢æŸ¥è³‡æ–™é›†ï¼"

# ğŸ”¹ 4. å®šç¾©åµŒå…¥å‡½æ•¸
def get_embedding(text):
    """
    ä½¿ç”¨ NLP æ¨¡å‹æå– XSS Payload å‘é‡
    """
    inputs = tokenizer(text, return_tensors="pt", padding=True, max_length=512, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    hidden_states = outputs.last_hidden_state
    sentence_embedding = hidden_states.mean(dim=1).squeeze().numpy()
    return sentence_embedding

# ğŸ”¹ 5. è½‰æ›æ‰€æœ‰ XSS Payload ç‚ºåµŒå…¥å‘é‡
print(f"ğŸš€ æ­£åœ¨ä½¿ç”¨ {model_name} è½‰æ› XSS Payload ...")
embeddings = np.array([get_embedding(text) for text in tqdm(payloads, desc="åµŒå…¥é€²åº¦")])

# ğŸ”¹ 6. åˆå§‹åŒ– FAISS å‘é‡åº«
embedding_dimension = embeddings.shape[1]
faiss_index = faiss.IndexFlatIP(embedding_dimension)  # å…§ç©ç›¸ä¼¼åº¦
# æ­£è¦åŒ–åµŒå…¥å‘é‡
normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
faiss_index.add(np.array(normalized_embeddings, dtype='float32'))


# ğŸ”¹ 7. å„²å­˜ FAISS ç´¢å¼•
faiss_index_file = os.path.join(model_output_dir, f"xss_vector_index_{model_filename}.faiss")
faiss.write_index(faiss_index, faiss_index_file)
print(f"âœ… å‘é‡åº«å·²å„²å­˜ç‚º: {faiss_index_file}")

# ğŸ”¹ 8. å„²å­˜æ¨™ç±¤èˆ‡åŸå§‹ Payload
np.save(os.path.join(model_output_dir, f"xss_labels_{model_filename}.npy"), labels)
np.save(os.path.join(model_output_dir, f"xss_payloads_{model_filename}.npy"), payloads)

print(f"âœ… æ¨™ç±¤èˆ‡ Payload å·²å„²å­˜ï¼ŒXSS FAISS è³‡æ–™åº«å»ºç«‹å®Œæˆï¼ğŸ‰")
