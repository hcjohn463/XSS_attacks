import os
import csv
import numpy as np
from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score
import matplotlib.pyplot as plt
from tqdm import tqdm
import time

# é¸æ“‡åµŒå…¥æ¨¡å‹
# model_name = "BAAI/bge-small-en"
model_name = "sentence-transformers/all-MiniLM-L6-v2"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

print(f"æ­£åœ¨ä½¿ç”¨ {model_name} æ¨¡å‹é€²è¡Œ XSS æª¢æ¸¬...")

# å–å¾—åµŒå…¥å‘é‡çš„å‡½æ•¸
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    
    # å–å¾—å¥å­åµŒå…¥ï¼ˆå¹³å‡æ± åŒ–ï¼‰
    hidden_states = outputs.last_hidden_state
    sentence_embedding = hidden_states.mean(dim=1).squeeze().numpy()
    return sentence_embedding

# è®€å–æ¸¬è©¦æ•¸æ“š
input_file = "D:/RAG/xss_attacks/dataset/XSS_dataset_testing_cleaned.csv"
print(f"ğŸ“¥ è®€å–æ¸¬è©¦æ•¸æ“š: {input_file}...")
results = []
true_labels = []
predicted_labels = []

data_count = 0
with open(input_file, "r", encoding="utf-8") as csvfile:
    reader = csv.DictReader(csvfile)
    data = list(reader)
    data_count = len(data)
    print(f"âœ… å…±è®€å–åˆ° {data_count} ç­† XSS æ¸¬è©¦æ•¸æ“šã€‚")

# è¨˜éŒ„æ•´é«”æ¸¬è©¦æ™‚é–“
start_time_total = time.time()

# è¨ˆç®—æ¯å€‹ Payload çš„åµŒå…¥å‘é‡ï¼Œä¸¦é€²è¡Œåˆ†é¡
for row in tqdm(data, desc="è™•ç†æ¸¬è©¦æ•¸æ“šé€²åº¦", unit="ç­†"):
    user_payload = row["Payload"]
    true_label = int(row["Label"])  # 0 = benign, 1 = malicious

    # å–å¾—åµŒå…¥å‘é‡
    start_time = time.perf_counter()
    payload_embedding = get_embedding(user_payload)
    inference_time_ms = (time.perf_counter() - start_time) * 1000  # è½‰æ›ç‚ºæ¯«ç§’

    # ä½¿ç”¨é–¾å€¼ä¾†åˆ¤æ–·æ˜¯å¦ç‚ºæƒ¡æ„ XSS
    threshold = 0.5  # å¯èª¿æ•´
    similarity_score = np.linalg.norm(payload_embedding)  # é€™è£¡å¯ä»¥æ›æˆ Cosine Similarity è¨ˆç®—
    predicted_label = 1 if similarity_score > threshold else 0  # è¨­å®šé–¾å€¼åˆ†é¡

    results.append({
        "payload": user_payload,
        "true_label": true_label,
        "predicted_label": predicted_label,
        "similarity_score": round(similarity_score, 4),
        "inference_time_ms": round(inference_time_ms, 4)
    })
    true_labels.append(true_label)
    predicted_labels.append(predicted_label)

# è¨˜éŒ„æ¸¬è©¦å®Œæˆæ™‚é–“
total_time = time.time() - start_time_total
average_time = (total_time / data_count) * 1000  # è½‰æ›ç‚ºæ¯«ç§’

# è¨­ç½®è¼¸å‡ºç›®éŒ„
base_output_dir = "D:/RAG/xss_attacks/result/direct"
model_output_dir = os.path.join(base_output_dir, model_name.replace('-', '_').replace('/', '_'))

# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(model_output_dir, exist_ok=True)

# è¨­ç½®è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
output_file = os.path.join(model_output_dir, f"testing_results_{model_name.replace('-', '_').replace('/', '_')}.csv")
confusion_matrix_file = os.path.join(model_output_dir, f"confusion_matrix_{model_name.replace('-', '_').replace('/', '_')}.png")
summary_file = os.path.join(model_output_dir, "summary_results.txt")

# å¯«å…¥çµæœåˆ° CSV
print(f"ğŸ“„ å¯«å…¥çµæœåˆ° {output_file}...")
with open(output_file, "w", newline="", encoding="utf-8") as csvfile:
    fieldnames = ["payload", "true_label", "predicted_label", "similarity_score", "inference_time_ms"]
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    writer.writerows(results)

print(f"âœ… çµæœå·²ä¿å­˜åˆ° {output_file}ï¼")

# è¨ˆç®— Accuracy, Precision, Recall
accuracy = accuracy_score(true_labels, predicted_labels) * 100
precision = precision_score(true_labels, predicted_labels) * 100
recall = recall_score(true_labels, predicted_labels) * 100

# è¨ˆç®—ç¸½æ™‚é–“æ ¼å¼åŒ–
total_minutes = int(total_time // 60)
total_seconds = int(total_time % 60)

# æ‰“å°çµæœ
print(f"ğŸ“Š Accuracy: {accuracy:.3f}%")
print(f"ğŸ“Š Precision: {precision:.3f}%")
print(f"ğŸ“Š Recall: {recall:.3f}%")
print(f"â±ï¸ Total Time: {total_minutes}min {total_seconds}sec")
print(f"â±ï¸ Average Time: {average_time:.2f}ms")

# ç¹ªè£½æ··æ·†çŸ©é™£
print("ğŸ“Š ç¹ªè£½æ··æ·†çŸ©é™£...")
cm = confusion_matrix(true_labels, predicted_labels, labels=[0, 1])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["benign", "malicious"])
disp.plot(cmap=plt.cm.Blues, colorbar=False, values_format='.0f')

# è¨­ç½®æ¨™é¡Œèˆ‡æ¨™ç±¤
plt.title(f"Confusion Matrix_{model_name.replace('-', '_').replace('/', '_')}")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")

# ä¿å­˜æ··æ·†çŸ©é™£åœ–åƒ
plt.savefig(confusion_matrix_file)
plt.show()

print(f"âœ… æ··æ·†çŸ©é™£å·²ä¿å­˜ç‚ºï¼š{confusion_matrix_file}")

# ç”Ÿæˆ Summary File
print(f"ğŸ“„ ç”Ÿæˆç¸½çµæ–‡ä»¶ {summary_file}...")
with open(summary_file, "w", encoding="utf-8") as f:
    f.write(f"Accuracy: {accuracy:.3f}%\n")
    f.write(f"Precision: {precision:.3f}%\n")
    f.write(f"Recall: {recall:.3f}%\n")
    f.write(f"Total Time: {total_minutes}min {total_seconds}sec\n")
    f.write(f"Average Time: {average_time:.2f}ms\n")

print(f"âœ… Summary å·²ä¿å­˜è‡³ {summary_file}ï¼")
