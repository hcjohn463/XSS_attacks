from transformers import AutoTokenizer, AutoModel
import faiss
import torch
import numpy as np
import time
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# ğŸ”¹ 1. è¨­å®š NLP æ¨¡å‹
# model_name = "BAAI/bge-small-en"  
# model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_name = 'microsoft/codebert-base'
# model_name = "jackaduma/SecBERT"
# model_name = "cssupport/mobilebert-sql-injection-detect"
# model_name = "roberta-base-openai-detector"




print(f"ğŸ” ä½¿ç”¨æ¨¡å‹: {model_name}")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
model_filename = model_name.replace('-', '_').replace('/', '_')

# ğŸ”¹ 3. åŠ è¼‰ FAISS å‘é‡ç´¢å¼• & æ¨™ç±¤
base_vector_dir = "../../dataset/vector"
model_vector_dir = os.path.join(base_vector_dir, model_filename)

index_file = os.path.join(model_vector_dir, f"xss_vector_index_{model_filename}.faiss")
labels_file = os.path.join(model_vector_dir, f"xss_labels_{model_filename}.npy")
payloads_file = os.path.join(model_vector_dir, f"xss_payloads_{model_filename}.npy")

# ğŸ”¹ 3. åŠ è¼‰ FAISS å‘é‡ç´¢å¼• & æ¨™ç±¤
print(f"ğŸ“¥ åŠ è¼‰ XSS å‘é‡åº«ï¼ˆ{index_file}ï¼‰...")
index = faiss.read_index(index_file)
labels = np.load(labels_file)
payloads = np.load(payloads_file, allow_pickle=True)

print(f"âœ… å‘é‡ç´¢å¼•ä¸­åŒ…å« {index.ntotal} æ¢ XSS Payloadsã€‚")

# ğŸ”¹ 4. å®šç¾© XSS Payload åµŒå…¥å‡½æ•¸
def get_embedding(text):
    """
    è½‰æ› XSS Payload æˆå‘é‡
    """
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    hidden_states = outputs.last_hidden_state
    sentence_embedding = hidden_states.mean(dim=1).squeeze().numpy()
    return sentence_embedding

# ğŸ”¹ 5. å®šç¾© XSS æª¢æ¸¬å‡½æ•¸
def classify_xss_risk(user_input, k=5):
    """
    æª¢ç´¢ XSS é¢¨éšªï¼Œä¸¦åˆ¤æ–·æ˜¯å¦ç‚ºéæ³• XSS æ”»æ“Šã€‚
    
    Args:
        user_input (str): ç”¨æˆ¶è¼¸å…¥çš„ HTML/JavaScript å…§å®¹ã€‚
        k (int): è¿”å›çš„æœ€ç›¸ä¼¼ XSS Payload æ•¸é‡ã€‚
    
    Returns:
        dict: åŒ…å« XSS é¢¨éšªè©•ä¼°çµæœèˆ‡è©³ç´°æª¢ç´¢è³‡è¨Šã€‚
    """
    start_time = time.perf_counter()
    print(f"\nğŸ› ï¸ æª¢æ¸¬ XSS é¢¨éšª: {user_input}\n")
    
    # åµŒå…¥ç”¨æˆ¶è¼¸å…¥
    input_embedding = get_embedding(user_input)
    
    # æª¢ç´¢ FAISS
    input_embedding = input_embedding / np.linalg.norm(input_embedding, keepdims=True)
    distances, indices = index.search(np.array([input_embedding], dtype="float32"), k)

    # è¨ˆç®—åˆ†æ•¸
    scores = {0: 0, 1: 0}
    valid_results = []
    for idx, dist in zip(indices[0], distances[0]):
        scores[labels[idx]] += dist
        valid_results.append({
            "index": int(idx), 
            "label": int(labels[idx]),  # 0=åˆæ³•, 1=éæ³•
            "distance": round(float(dist), 4),
            "payload": payloads[idx]  # XSS Payload
        })
    
    # åˆ¤æ–·åˆæ³•æ€§
    threshold = 0.7  # è¨­å®šç›¸ä¼¼åº¦é–€æª»
    max_score = max(scores[0], scores[1])  # å–æœ€é«˜çš„ç›¸ä¼¼åº¦åˆ†æ•¸

    if max_score < threshold:
        legality = "åˆæ³•"  # å¦‚æœæœ€é«˜ç›¸ä¼¼åº¦ä½æ–¼é–¾å€¼ï¼Œåˆ¤å®šç‚ºåˆæ³•
    else:
        legality = "éæ³•" if scores[1] > scores[0] else "åˆæ³•"

    # è¨ˆç®—æ¨è«–æ™‚é–“
    inference_time_ms = (time.perf_counter() - start_time) * 1000

    return {
        "input_payload": user_input,
        "legality": legality,
        "reason": f"Scores: {{'åˆæ³•': {scores[0]:.4f}, 'éæ³•': {scores[1]:.4f}}}",
        "details": valid_results,
        "inference_time_ms": inference_time_ms
    }

# ğŸ”¹ 6. è¨­ç½® `k` å€¼
k_value = 3

# ğŸ”¹ 7. å¾ªç’°è¼¸å…¥æ¸¬è©¦ XSS Payload
while True:
    user_query = input("è«‹è¼¸å…¥ XSS Payloadï¼ˆæˆ–è¼¸å…¥ 'exit' çµæŸï¼‰ï¼š")
    if user_query.lower() == 'exit':
        print("ğŸš€ çµæŸ XSS æª¢æ¸¬ç¨‹åºã€‚")
        break

    result = classify_xss_risk(user_query, k=k_value)

    # ğŸ”¹ 8. è¼¸å‡ºçµæœ
    print("\nğŸ“Š æª¢æ¸¬çµæœï¼š")
    print(f"ğŸ“ è¼¸å…¥ Payload: {user_query}")
    print(f"ğŸ” åˆ¤æ–·çµæœ: {result['legality']}")
    print(f"ğŸ› ï¸ åˆ¤æ–·ä¾æ“š: {result['reason']}")
    print(f"â±ï¸ æ¨è«–æ™‚é–“: {result['inference_time_ms']:.4f} ms")
    print(f"\nğŸ” æœ€ç›¸ä¼¼çš„ {k_value} å€‹ XSS Payloadsï¼š")
    for i, res in enumerate(result["details"], start=1):
        print(f"{i}. [ç´¢å¼• {res['index']}] XSS: {res['payload']} (æ¨™ç±¤: {res['label']}, è·é›¢: {res['distance']})")

print("\nâœ… XSS æª¢æ¸¬å®Œæˆï¼Œç¨‹åºçµæŸï¼ğŸš€")
